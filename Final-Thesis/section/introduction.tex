\documentclass[../thesis.tex]{subfiles}
\begin{document}

\chapter{Introduction}
\label{ch:intro}

A network's backbone is its routing control plane: a set of rules and distributed routing protocols that describe how the network should operate. A control plane is thus defined through configuration files present on every individual routing device in the network. These configurations are written in vendor specific languages (e.g. Cisco IOS and Juniper Junos) and describe very low level behaviours of a particular router. Network operators tasked to configure control planes are required to satisfy various `policies' that the owning organization wants to enforce: e.g certain devices should always be blocked from communicating with higher privileged devices.\\

Research has shown that configuring control planes can be extremely complex in modern networks~\cite{complexity}. Consequently, this causes configurations to be prone to errors, many of which are only uncovered during operation after a failure has already dealt significant damage ~\cite{errors}.â€¯For example in 2012, failure of a router in a Microsoft Azure data center triggered previously unknown configuration errors on other devices, degrading service in the West Europe region for over two hours~\cite{azure}. Additionally, surveys done by researchers in the past~\cite{Zeng} have shown that network administrators report router/switch software bugs as the most common symptoms of network failure. Similarly, ~\cite{Lee} discovered a number of errors in the routing policies applied to BGP sessions. More specifically they observed missing BGP communities, typos in the definitions of existing communities and missing import or export policy. These examples highlight a need to develop highly resilient configurations that perform reliably.\\  


\section{Existing Tools}

Even though networks are getting larger and more complex, administrators use simple tools for configuring network devices, such as CLIs on the routing devices or networking management tools such as NetMRI or SolarWinds (further described in section 3.3). Additionally, it is notoriously hard to debug problems in a network; operators often have to rely on rudimentary tools such as ping and traceroute. Apart from these existing tools, there are two main areas of research that are trying to enable network resilience: configuration synthesis and network verification. Synthesis tools like NetComplete~\cite{NetComplete}, Zeppelin and SyNet~\cite{synet} are working towards perhaps the most ambitious solution: automatically generating all configurations in one batch. However, all synthesis tools require comprehensive input from the network operators. They usually have to fully specify the high level requirements of the policies the organization wishes to impose, ahead of time. This can be a time consuming and nuanced task for the operators. It is difficult to ascertain what existing policies the network follows and there is a lot of room to miss some rule or policy.\\

This high-input nature of synthesis systems makes them much more suitable during the initial stages of developing a network, given that the operators are willing to put in the work. However, the return rate of configuration synthesis falls sharply if the network operators are concerned with incremental changes. Whenever there is a new router being added to the network, the networking policies are essentially changed in accordance with this new router. Thus network operators will have to re-run the synthesis system to generate configurations for the entire network regardless of whether a router needs to be updated or not. This can make synthesis an unnecessarily time consuming process as all routers have to be updated, which may lead to more downtime in the network.\\ 

In contrast, network verification and group repair with synthesis (e.g ARC~\cite{arc}) can be extremely useful to determine whether the current configurations comply with the existing policies of the network. Again, these policies could be defined by the operators or, as in the case of ARC, they could be inferred via a snapshot of the network. However, verification by design is a post development tool i.e. it will catch errors after they have already been produced. It does not help operators avoid producing these errors in the first place. Furthermore, even though it is possible for certain tools (like ARC) to generate a possible fix, it is not a guarantee made by verification tools in general. In essence, the operators will have to retrace the error and manually edit the faulty configurations. We thus wish to explore a solution that sits in between configuration synthesis and verification.\\

One existing approach that can be utilized by network operators in the middle of the development lifecycle is templating. Currently, operators try to minimize extraneous work by reusing existing configurations that have been known to work in the past. When creating a network, operators typically write templates containing specific configuration lines that define a base set of behaviours for different router roles~\cite{complexity}. These templates are then used to specialize individual routers to achieve objectives for their respective part of the network. Due to varying router specifications, the template systems used allow network operators to fill in parameters with appropriate information each time the template is used. However in practice, as the customer requirements of the organization change over time, the networks start to get more complex. ~\cite{Benson} describes how configuration templates in use constantly grow in number and become increasingly specialized as a service grows and as customer requirements evolve. The paper shows how operators will have to write up new templates or edit existing ones eventually. We envision our engine would ease this process by providing convenient suggestions while operators are editing configurations (regardless of whether they are templates or not).
Our approach thus serves to complement existing techniques for writing routing configurations.\\ 

\section{Motivation for a Completion Engine}

The analyses on two campus networks performed by~\cite{Kim} shed light on how networks evolve. If we consider the types of changes that operators perform, we observe some homogeneity. Not only are many changes security related but, each campus has distinct security practices that use specific router configuration commands. This seems to hold true across organizations, where certain design and operational practices tend to be followed. Thus, a completion engine could exploit this regularity by learning from a history of changes and suggest the routines and practices that an organization tends to follow. Our penultimate goal would be a "writing assistant" for network configurations, one that can complete entire stanzas (explained in section 2.1) and recommend more concise syntax. It could also proactively offer "negative" recommendations. If the operator attempts to change a line or stanza of the network configuration that is not frequently changed, the system could discourage the modification by alerting the operator. We postulate that developing an effective token recommendation system would be a concrete first step towards such a goal.\\

We consider the problem of writing network configurations to be analogous to writing software code. Most configurations are written using vendor specific languages, that make use of rules and keywords similar to traditional programming languages. We envision an interactive system inspired by code completion engines that could be invoked by network operators as they are writing router configurations to offer them suggestions for what to put in next, or list the options available from the invocation point. 

\section{Challenges}
There are a few challenges that arise when undertaking such a project. Firstly, routers tend to play different roles in the network. For example in our dataset, two common roles that we encountered were "core" and "edge". Core routers are designed to form the backbone of the networks and are configured to handle connections between all the devices within the network. Edge routers can be thought of sitting on the edge of this backbone, connecting multiple core routers to each other and also to routers outside the organization. Since the roles vary in configuration, our main concern was whether we could build a model that could cater to all the roles in the network.\\

Secondly, routing configurations tend to have a lot of parameters such as IP addresses, subnet masks, interface names etc. that add a lot of variance between configurations. We would thus need a way to clean our data and ensure that we can reasonably suggest a generic parameter type without trying to guess the exact value. Another challenge is minimizing the size of data required for the model. If an engine requires copious amounts of data, then it might be unsuitable to be used by some organizations who do not regularly store snapshots of their network's configurations. The model should thus perform reasonably accurately in situations of data scarcity. 

We explain how our model addresses some of these challenges in section 4. In general, we saw that using a Natural Language Processing approach helped answer a lot of our challenges.

\section{Completion Using NLP}
Recent research on software systems has shown that codebases tend to contain regularities, much like natural languages~\cite{naturalness}. This has motivated further research on using traditional Natural Language Processing techniques for code completion and token suggestion, resulting in fairly accurate models~\cite{naturalness, raychev}. We hypothesize a similar regularity for network configurations, especially since they tend to be homogeneous by design, reusing the same set of keywords/tokens. In Section 3.1, our analysis of router configurations from a large research university showed that configurations shared between 85\% and 99\% of tokens across different routers, which seems to support our claim. This prompted us to explore simple NLP techniques that could leverage these token similarities to produce useful suggestions or completions.\\

Another reason we gravitated towards NLP as a basis of our model is the flexibility it entails. Traditional code-completion techniques capitalize on the grammar rules of the languages to build their models. An analogous version for network configurations would require us to reverse engineer the configuration grammar, presumably exploiting some existing parser.\footnote{In fact, we tried to extract this information ourselves, hoping to compare our model to tab-completion which seemed as a reasonable ground truth to try and improve on. However, accurately parsing the parse tree proved to be a tedious, time consuming process. We eventually had to drop this line of analysis, as we were unable to get the system to a point where we were satisfied by the tab-completion output.}
Moreover, as different vendors have different grammars, we would effectively have to repeat this process for every vendor and ostensibly rebuild the engine. On the other hand, an NLP approach is theoretically language agnostic. For example, in this paper we perform tests on Cisco configurations but, we could easily train and test the model on configurations written for Juniper. Conceding that it is almost exclusively data dependent, we maintain that an NLP model is a good candidate for an elegant solution to the token completion problem. We thus consider our work as a feasibility analysis for this particular strategy. In general, our results show that using an off-the-shelf NLP algorithm with minor modifications can give us up to 93\% accuracy for some configurations. These are encouraging results and in Section 4 we discuss some additional analyses we performed that make us optimistic about the utility of this approach.\\

\section{Organization}

This paper outlines the necessary details for building a completion engine for network configurations. Section ~\ref{ch:background} provides background information about network configurations, existing network management tools and code completion techniques. Section \ref{ch:related} briefly discusses all the work pertinent to our research. Section \ref{ch:prelim} describes our token analysis results and explains how our model works. Section \ref{ch:experiments} and \ref{ch:results} describe and discuss all the analyses we performed using our model, followed by a conclusion in Section \ref{ch:conclusion}.

\end{document}
